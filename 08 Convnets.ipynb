{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, backend as K\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import graphviz\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load data from data.npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all numpy arrays except training images (in order to save memory).\n",
    "def load_data():\n",
    "    with np.load('data.npz', allow_pickle=True) as npz_file:\n",
    "        #X_train = npz_file['X_train']\n",
    "        X_valid = npz_file['X_valid']\n",
    "        X_test = npz_file['X_test']\n",
    "        X_train_features = npz_file['X_train_features']\n",
    "        X_valid_features = npz_file['X_valid_features']\n",
    "        X_test_features = npz_file['X_test_features']\n",
    "        y_train_1h = npz_file['y_train_1h']\n",
    "        y_valid_1h = npz_file['y_valid_1h']\n",
    "        y_test_1h = npz_file['y_test_1h']\n",
    "        y_train = npz_file['y_train']\n",
    "        y_valid = npz_file['y_valid']\n",
    "        y_test = npz_file['y_test']\n",
    "        class_indices = npz_file['class_indices']\n",
    "        train_filenames = npz_file['train_filenames']\n",
    "        valid_filenames = npz_file['valid_filenames']\n",
    "        test_filenames = npz_file['test_filenames']\n",
    "    return X_train_features, y_train_1h, y_train, train_filenames, X_valid,X_valid_features, y_valid_1h, y_valid, valid_filenames, X_test, X_test_features, y_test_1h, y_test, test_filenames,class_indices\n",
    "\n",
    "# load training images\n",
    "def load_images():\n",
    "    with np.load('data.npz', allow_pickle=True) as npz_file:\n",
    "        X_train = npz_file['X_train']\n",
    "    return X_train\n",
    "\n",
    "# merge two dictionaries\n",
    "def merge_dict(x,y):\n",
    "    d = x.copy()\n",
    "    d.update(y)\n",
    "    return d\n",
    "\n",
    "# fit a model\n",
    "def fit_model(model, grid, X_tr, y_tr, X_va, y_va, X_t, y_t):\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    params = []\n",
    "    for params_dict in grid:\n",
    "        #print (params_dict)\n",
    "        params.append(params_dict)\n",
    "        model.set_params(**params_dict)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_scores.append(model.score(X_tr, y_tr))\n",
    "        valid_scores.append(model.score(X_va, y_va))\n",
    "    best_index = np.argmax(valid_scores)\n",
    "    # refit model with best params\n",
    "    model.set_params(**params[best_index])\n",
    "    model.fit(X_tr, y_tr)\n",
    "    test_score = model.score(X_t, y_t)\n",
    "    return { 'best_params':params[best_index],\n",
    "              'params':params,\n",
    "             'train_scores':train_scores,\n",
    "             'valid_scores':valid_scores,\n",
    "             'test_score':test_score,\n",
    "             'best_index':best_index,\n",
    "              'best_valid_score':valid_scores[best_index],\n",
    "              'best_model':model\n",
    "            }\n",
    "\n",
    "X_train_features, y_train_1h, y_train, train_filenames, X_valid,X_valid_features, y_valid_1h, y_valid, valid_filenames, X_test, X_test_features, y_test_1h, y_test, test_filenames,class_indices = load_data()\n",
    "\n",
    "X_train = load_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network\n",
    "You tested above different models with the set of high-level features extracted from a\n",
    "pretrained neural network. However, can you get similar results by (re)training a\n",
    "ConvNet from the pixels?\n",
    "- What accuracy can you achieve?\n",
    "\n",
    "We will implement a simple ConvNet because we have very small training dataset :\n",
    "\n",
    "- conv2D layer : 16 kernels of size 5x5, 2x2 stride, \"same\" padding and ReLU activation\n",
    "- MaxPooling2D : 2x2 size and stride\n",
    "- conv2D layer : 16 kernels of size 3x3, 1x1 stride, \"same\" padding and ReLU activation\n",
    "- MaxPooling2D : 2x2 size and stride\n",
    "- Reshaping of the last layer to a 1-dimensional flat array.\n",
    "- Fully-connected layer with 2048 outputs.\n",
    "- Fully-connected layer with 6 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 299, 299, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 150, 150, 16)      1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 21904)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              44861440  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 44,877,270\n",
      "Trainable params: 44,877,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(keras.layers.Conv2D(16, kernel_size=(5, 5), strides=(2, 2),padding='same', activation='relu',\n",
    "                 input_shape=(299,299,3)))\n",
    "model_cnn.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_cnn.add(keras.layers.Conv2D(16, (3, 3), strides=(1, 1),padding='same',activation='relu'))\n",
    "model_cnn.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_cnn.add(keras.layers.Flatten())\n",
    "model_cnn.add(Dense(2048, activation='relu'))\n",
    "model_cnn.add(Dense(6, activation='softmax'))\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compile and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 49s 35ms/step - loss: 2.8584 - acc: 0.2186 - val_loss: 1.6761 - val_acc: 0.3309\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 42s 30ms/step - loss: 1.5727 - acc: 0.3821 - val_loss: 1.4481 - val_acc: 0.4460\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 37s 27ms/step - loss: 1.2816 - acc: 0.5264 - val_loss: 1.3879 - val_acc: 0.4892\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 37s 26ms/step - loss: 0.7423 - acc: 0.7600 - val_loss: 1.3249 - val_acc: 0.5036\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 39s 28ms/step - loss: 0.2898 - acc: 0.9236 - val_loss: 1.6353 - val_acc: 0.4964\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 36s 25ms/step - loss: 0.0608 - acc: 0.9900 - val_loss: 1.9769 - val_acc: 0.4964\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 35s 25ms/step - loss: 0.0127 - acc: 0.9993 - val_loss: 2.1383 - val_acc: 0.4604\n",
      "Epoch 8/50\n",
      "1400/1400 [==============================] - 37s 26ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 2.2528 - val_acc: 0.4532\n",
      "Epoch 9/50\n",
      "1400/1400 [==============================] - 35s 25ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.3603 - val_acc: 0.4676\n",
      "50/50 [==============================] - 0s 8ms/step\n",
      "Test Accuracy : 0.5600000023841858\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "cnn_fit = model_cnn.fit(\n",
    "                        x=X_train, y=y_train_1h,\n",
    "                        validation_data=(X_valid, y_valid_1h), batch_size=32, epochs=50,\n",
    "                        callbacks=[keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)]\n",
    "                        ,shuffle=True\n",
    "                        )\n",
    "(test_loss, test_accuracy) = model_cnn.evaluate(X_test, y_test_1h, batch_size=100)\n",
    "print(\"Test Accuracy :\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute it's accuracy on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 6ms/step\n",
      "Test Accuracy : 0.5600000023841858\n"
     ]
    }
   ],
   "source": [
    "(test_loss, test_accuracy) = model_cnn.evaluate(X_test, y_test_1h, batch_size=100)\n",
    "print(\"Test Accuracy :\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy is very low.\n",
    "\n",
    "- Can you get good results? - If not, why?\n",
    "\n",
    "Our cnn model has about 45 million parameters! input space dimension is 299x299x3 = 268203! and our training set has only 1400 rows!\n",
    "\n",
    "Training our model with few data will not generalize well. We need much more data to train our model.\n",
    "\n",
    "Using transfer learning, as we have done in this project, is essential when we have very little data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
