{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Dense networks</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, backend as K\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import graphviz\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load data from data.npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all numpy arrays except training images (in order to save memory).\n",
    "def load_data():\n",
    "    with np.load('data.npz', allow_pickle=True) as npz_file:\n",
    "        #X_train = npz_file['X_train']\n",
    "        X_valid = npz_file['X_valid']\n",
    "        X_test = npz_file['X_test']\n",
    "        X_train_features = npz_file['X_train_features']\n",
    "        X_valid_features = npz_file['X_valid_features']\n",
    "        X_test_features = npz_file['X_test_features']\n",
    "        y_train_1h = npz_file['y_train_1h']\n",
    "        y_valid_1h = npz_file['y_valid_1h']\n",
    "        y_test_1h = npz_file['y_test_1h']\n",
    "        y_train = npz_file['y_train']\n",
    "        y_valid = npz_file['y_valid']\n",
    "        y_test = npz_file['y_test']\n",
    "        class_indices = npz_file['class_indices']\n",
    "        train_filenames = npz_file['train_filenames']\n",
    "        valid_filenames = npz_file['valid_filenames']\n",
    "        test_filenames = npz_file['test_filenames']\n",
    "    return X_train_features, y_train_1h, y_train, train_filenames, X_valid,X_valid_features, y_valid_1h, y_valid, valid_filenames, X_test, X_test_features, y_test_1h, y_test, test_filenames,class_indices\n",
    "\n",
    "# load training images\n",
    "def load_images():\n",
    "    with np.load('data.npz', allow_pickle=True) as npz_file:\n",
    "        X_train = npz_file['X_train']\n",
    "    return X_train\n",
    "\n",
    "# merge two dictionaries\n",
    "def merge_dict(x,y):\n",
    "    d = x.copy()\n",
    "    d.update(y)\n",
    "    return d\n",
    "\n",
    "# fit a model\n",
    "def fit_model(model, grid, X_tr, y_tr, X_va, y_va, X_t, y_t):\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    params = []\n",
    "    for params_dict in grid:\n",
    "        #print (params_dict)\n",
    "        params.append(params_dict)\n",
    "        model.set_params(**params_dict)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_scores.append(model.score(X_tr, y_tr))\n",
    "        valid_scores.append(model.score(X_va, y_va))\n",
    "    best_index = np.argmax(valid_scores)\n",
    "    # refit model with best params\n",
    "    model.set_params(**params[best_index])\n",
    "    model.fit(X_tr, y_tr)\n",
    "    test_score = model.score(X_t, y_t)\n",
    "    return { 'best_params':params[best_index],\n",
    "              'params':params,\n",
    "             'train_scores':train_scores,\n",
    "             'valid_scores':valid_scores,\n",
    "             'test_score':test_score,\n",
    "             'best_index':best_index,\n",
    "              'best_valid_score':valid_scores[best_index],\n",
    "              'best_model':model\n",
    "            }\n",
    "\n",
    "X_train_features, y_train_1h, y_train, train_filenames, X_valid,X_valid_features, y_valid_1h, y_valid, valid_filenames, X_test, X_test_features, y_test_1h, y_test, test_filenames,class_indices = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense network\n",
    "Finally, try with neural networks\n",
    "- layer dense network i.e. no hidden layer, just the input and output ones\n",
    "\n",
    "This is an example of 1 layer neural network for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 12,294\n",
      "Trainable params: 12,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_1l_model = Sequential()\n",
    "nn_1l_model.add(Dense(6, activation='softmax', input_dim=X_train_features.shape[1],\n",
    "                     kernel_initializer=keras.initializers.VarianceScaling(scale=1.0, seed=0),\n",
    "                     kernel_regularizer=keras.regularizers.l2(10**1),\n",
    "                     activity_regularizer=keras.regularizers.l1(10**1)\n",
    "                     )\n",
    "               )\n",
    "nn_1l_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try fitting our model with regularization using a grid of C parameters. We will also stop training when there is overfitting by using EarlyStopping callback.\n",
    "Training will stop when validation accuracy starts to go down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 7s 5ms/step - loss: 0.4321 - acc: 0.8686 - val_loss: 0.3634 - val_acc: 0.9065\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1096 - acc: 0.9793 - val_loss: 0.3914 - val_acc: 0.8777\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 1s 572us/step - loss: 0.0731 - acc: 0.9950 - val_loss: 0.4002 - val_acc: 0.8993\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 1s 584us/step - loss: 0.0599 - acc: 0.9986 - val_loss: 0.4009 - val_acc: 0.8993\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 1s 586us/step - loss: 0.0535 - acc: 1.0000 - val_loss: 0.3996 - val_acc: 0.8993\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 1s 579us/step - loss: 0.0502 - acc: 1.0000 - val_loss: 0.4101 - val_acc: 0.9065\n",
      "50/50 [==============================] - 0s 236us/step\n",
      "Test Accuracy : 0.9599999785423279\n",
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 2s 2ms/step - loss: 0.7766 - acc: 0.8621 - val_loss: 0.6839 - val_acc: 0.8921\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 1s 598us/step - loss: 0.4597 - acc: 0.9750 - val_loss: 0.7331 - val_acc: 0.9137\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 1s 654us/step - loss: 0.4197 - acc: 0.9957 - val_loss: 0.7078 - val_acc: 0.9137\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 1s 589us/step - loss: 0.4044 - acc: 0.9979 - val_loss: 0.7188 - val_acc: 0.9065\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 1s 616us/step - loss: 0.3949 - acc: 1.0000 - val_loss: 0.7008 - val_acc: 0.9137\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 1s 680us/step - loss: 0.3890 - acc: 1.0000 - val_loss: 0.7053 - val_acc: 0.9137\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 1s 680us/step - loss: 0.3837 - acc: 1.0000 - val_loss: 0.6986 - val_acc: 0.9137\n",
      "50/50 [==============================] - 0s 215us/step\n",
      "Test Accuracy : 0.9599999785423279\n",
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 2s 2ms/step - loss: 4.1435 - acc: 0.8636 - val_loss: 3.8177 - val_acc: 0.9065\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 1s 639us/step - loss: 3.6574 - acc: 0.9757 - val_loss: 3.6412 - val_acc: 0.9065\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 1s 632us/step - loss: 3.4795 - acc: 0.9914 - val_loss: 3.5044 - val_acc: 0.8993\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 1s 694us/step - loss: 3.3933 - acc: 0.9929 - val_loss: 3.4212 - val_acc: 0.9137\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 1s 653us/step - loss: 3.3540 - acc: 0.9964 - val_loss: 3.3958 - val_acc: 0.9065\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 1s 608us/step - loss: 3.3377 - acc: 0.9929 - val_loss: 3.3955 - val_acc: 0.9137\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 1s 650us/step - loss: 3.3326 - acc: 0.9950 - val_loss: 3.3923 - val_acc: 0.8993\n",
      "Epoch 8/50\n",
      "1400/1400 [==============================] - 1s 700us/step - loss: 3.3273 - acc: 0.9971 - val_loss: 3.4016 - val_acc: 0.8993\n",
      "Epoch 9/50\n",
      "1400/1400 [==============================] - 1s 674us/step - loss: 3.3275 - acc: 0.9936 - val_loss: 3.3642 - val_acc: 0.9137\n",
      "50/50 [==============================] - 0s 31us/step\n",
      "Test Accuracy : 0.9399999976158142\n",
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 2s 2ms/step - loss: 35.0352 - acc: 0.8486 - val_loss: 31.1973 - val_acc: 0.8705\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 1s 604us/step - loss: 32.3919 - acc: 0.9600 - val_loss: 30.8380 - val_acc: 0.8777\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 1s 644us/step - loss: 32.3174 - acc: 0.9457 - val_loss: 30.8757 - val_acc: 0.8993\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 1s 652us/step - loss: 32.3109 - acc: 0.9579 - val_loss: 30.8378 - val_acc: 0.9137\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 1s 646us/step - loss: 32.3320 - acc: 0.9471 - val_loss: 30.8359 - val_acc: 0.9137\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 1s 687us/step - loss: 32.3247 - acc: 0.9479 - val_loss: 30.8641 - val_acc: 0.9137\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 1s 601us/step - loss: 32.3370 - acc: 0.9429 - val_loss: 30.8620 - val_acc: 0.8921\n",
      "Epoch 8/50\n",
      "1400/1400 [==============================] - 1s 660us/step - loss: 32.3218 - acc: 0.9486 - val_loss: 30.9024 - val_acc: 0.8849\n",
      "Epoch 9/50\n",
      "1400/1400 [==============================] - 1s 608us/step - loss: 32.3194 - acc: 0.9557 - val_loss: 30.8357 - val_acc: 0.9281\n",
      "Epoch 10/50\n",
      "1400/1400 [==============================] - 1s 666us/step - loss: 32.3233 - acc: 0.9529 - val_loss: 30.8914 - val_acc: 0.9065\n",
      "Epoch 11/50\n",
      "1400/1400 [==============================] - 1s 737us/step - loss: 32.3267 - acc: 0.9464 - val_loss: 30.8580 - val_acc: 0.9137\n",
      "Epoch 12/50\n",
      "1400/1400 [==============================] - 1s 580us/step - loss: 32.3255 - acc: 0.9457 - val_loss: 30.8918 - val_acc: 0.9065\n",
      "Epoch 13/50\n",
      "1400/1400 [==============================] - 1s 638us/step - loss: 32.3318 - acc: 0.9443 - val_loss: 30.8637 - val_acc: 0.8993\n",
      "Epoch 14/50\n",
      "1400/1400 [==============================] - 1s 605us/step - loss: 32.3249 - acc: 0.9543 - val_loss: 30.8613 - val_acc: 0.8921\n",
      "50/50 [==============================] - 0s 232us/step\n",
      "Test Accuracy : 0.9399999976158142\n",
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 2s 2ms/step - loss: 337.2848 - acc: 0.7164 - val_loss: 305.2385 - val_acc: 0.8849\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 1s 710us/step - loss: 319.8548 - acc: 0.9079 - val_loss: 304.3879 - val_acc: 0.9065\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 1s 704us/step - loss: 319.6867 - acc: 0.9064 - val_loss: 304.4038 - val_acc: 0.8633\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 1s 603us/step - loss: 319.6870 - acc: 0.9000 - val_loss: 304.4019 - val_acc: 0.8705\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 1s 649us/step - loss: 319.6891 - acc: 0.8971 - val_loss: 304.4190 - val_acc: 0.8777\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 1s 699us/step - loss: 319.6923 - acc: 0.9029 - val_loss: 304.4337 - val_acc: 0.8417\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 1s 690us/step - loss: 319.7009 - acc: 0.8850 - val_loss: 304.4031 - val_acc: 0.8705\n",
      "50/50 [==============================] - 0s 264us/step\n",
      "Test Accuracy : 0.9399999976158142\n",
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 3360.0813 - acc: 0.4071 - val_loss: 3043.4417 - val_acc: 0.8201\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 1s 605us/step - loss: 3189.4997 - acc: 0.8264 - val_loss: 3035.4838 - val_acc: 0.8201\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 1s 616us/step - loss: 3187.9369 - acc: 0.8193 - val_loss: 3035.4171 - val_acc: 0.7986\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 1s 640us/step - loss: 3187.9172 - acc: 0.7950 - val_loss: 3035.4250 - val_acc: 0.8058\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 1s 728us/step - loss: 3187.9158 - acc: 0.7857 - val_loss: 3035.4154 - val_acc: 0.8058\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 1s 706us/step - loss: 3187.9147 - acc: 0.7793 - val_loss: 3035.4150 - val_acc: 0.8058\n",
      "50/50 [==============================] - 0s 252us/step\n",
      "Test Accuracy : 0.8199999928474426\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "scaler = StandardScaler()\n",
    "X_train_preprocessed = scaler.fit_transform(X_train_features).astype(float)\n",
    "X_valid_preprocessed = scaler.fit_transform(X_valid_features).astype(float)\n",
    "X_test_preprocessed = scaler.transform(X_test_features).astype(float)\n",
    "nn_test=[]\n",
    "for C in [10**i for i in range(-3,3)]:\n",
    "    nn_1l_model = Sequential()\n",
    "    nn_1l_model.add(Dense(6, activation='softmax', input_dim=X_train_features.shape[1],\n",
    "                         kernel_initializer=keras.initializers.VarianceScaling(scale=1.0, seed=0),\n",
    "                         kernel_regularizer=keras.regularizers.l2(C),\n",
    "                         activity_regularizer=keras.regularizers.l1(C)\n",
    "                         )\n",
    "                   )\n",
    "    nn_1l_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    nn_1l_fit = nn_1l_model.fit(\n",
    "                            x=X_train_preprocessed, y=y_train_1h,\n",
    "                            validation_data=(X_valid_preprocessed, y_valid_1h), batch_size=32, epochs=50,\n",
    "                            callbacks=[keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)]\n",
    "                            ,shuffle=True\n",
    "                            )\n",
    "    (test_loss, test_accuracy) = nn_1l_model.evaluate(X_test_preprocessed, y_test_1h, batch_size=100)\n",
    "    nn_test.append((C,test_accuracy))\n",
    "    print(\"Test Accuracy :\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001, 0.9599999785423279),\n",
       " (0.01, 0.9599999785423279),\n",
       " (0.1, 0.9399999976158142),\n",
       " (1, 0.9399999976158142),\n",
       " (10, 0.9399999976158142),\n",
       " (100, 0.8199999928474426)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best test accuracy for this model is 96% for C=0.01 and 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- layer dense network i.e. one hidden layer\n",
    "\n",
    "For this 2 layers nn, we will do reguralization using dropout instead of l2 regularization.\n",
    "We will try many number of units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.7078 - acc: 0.7536 - val_loss: 0.2473 - val_acc: 0.8993\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 1s 688us/step - loss: 0.2501 - acc: 0.9121 - val_loss: 0.2555 - val_acc: 0.9065\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 1s 534us/step - loss: 0.1765 - acc: 0.9386 - val_loss: 0.2650 - val_acc: 0.9209\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 1s 636us/step - loss: 0.1336 - acc: 0.9529 - val_loss: 0.2615 - val_acc: 0.9137\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 1s 619us/step - loss: 0.0979 - acc: 0.9679 - val_loss: 0.2818 - val_acc: 0.9137\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 1s 580us/step - loss: 0.1046 - acc: 0.9650 - val_loss: 0.2704 - val_acc: 0.9137\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 1s 593us/step - loss: 0.0826 - acc: 0.9757 - val_loss: 0.3124 - val_acc: 0.9065\n",
      "Epoch 8/50\n",
      "1400/1400 [==============================] - 1s 622us/step - loss: 0.0671 - acc: 0.9800 - val_loss: 0.3190 - val_acc: 0.8993\n",
      "50/50 [==============================] - 0s 190us/step\n",
      "Test Accuracy : 0.8999999761581421\n",
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.5328 - acc: 0.8143 - val_loss: 0.2438 - val_acc: 0.9065\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 1s 679us/step - loss: 0.1606 - acc: 0.9479 - val_loss: 0.2480 - val_acc: 0.9137\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 1s 673us/step - loss: 0.1030 - acc: 0.9679 - val_loss: 0.2836 - val_acc: 0.9209\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 1s 579us/step - loss: 0.0688 - acc: 0.9736 - val_loss: 0.2956 - val_acc: 0.9353\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 1s 705us/step - loss: 0.0430 - acc: 0.9886 - val_loss: 0.3171 - val_acc: 0.8993\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 1s 701us/step - loss: 0.0426 - acc: 0.9836 - val_loss: 0.3195 - val_acc: 0.9137\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 1s 707us/step - loss: 0.0366 - acc: 0.9893 - val_loss: 0.3812 - val_acc: 0.9137\n",
      "Epoch 8/50\n",
      "1400/1400 [==============================] - 1s 697us/step - loss: 0.0286 - acc: 0.9929 - val_loss: 0.3599 - val_acc: 0.9209\n",
      "Epoch 9/50\n",
      "1400/1400 [==============================] - 1s 645us/step - loss: 0.0190 - acc: 0.9950 - val_loss: 0.3764 - val_acc: 0.9281\n",
      "50/50 [==============================] - 0s 270us/step\n",
      "Test Accuracy : 0.9200000166893005\n",
      "Train on 1400 samples, validate on 139 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 25s 18ms/step - loss: 0.4576 - acc: 0.8364 - val_loss: 0.2965 - val_acc: 0.8921\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 1s 471us/step - loss: 0.1135 - acc: 0.9621 - val_loss: 0.2982 - val_acc: 0.9065\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 0s 227us/step - loss: 0.0467 - acc: 0.9843 - val_loss: 0.2861 - val_acc: 0.9137\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 0s 264us/step - loss: 0.0374 - acc: 0.9864 - val_loss: 0.3260 - val_acc: 0.9065\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 0s 216us/step - loss: 0.0221 - acc: 0.9936 - val_loss: 0.3546 - val_acc: 0.9281\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 0s 217us/step - loss: 0.0156 - acc: 0.9957 - val_loss: 0.3420 - val_acc: 0.8921\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 0s 252us/step - loss: 0.0109 - acc: 0.9986 - val_loss: 0.3706 - val_acc: 0.8993\n",
      "Epoch 8/50\n",
      "1400/1400 [==============================] - 0s 255us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.3965 - val_acc: 0.9065\n",
      "Epoch 9/50\n",
      "1400/1400 [==============================] - 0s 302us/step - loss: 0.0156 - acc: 0.9957 - val_loss: 0.3547 - val_acc: 0.9137\n",
      "Epoch 10/50\n",
      "1400/1400 [==============================] - 0s 283us/step - loss: 0.0105 - acc: 0.9971 - val_loss: 0.3777 - val_acc: 0.9209\n",
      "50/50 [==============================] - 0s 529us/step\n",
      "Test Accuracy : 0.9200000166893005\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "nn2_test=[]\n",
    "for units in [32,64,128]:\n",
    "    nn_2l_model = Sequential()\n",
    "    # first layer : relu\n",
    "    nn_2l_model.add(Dense(units=units, activation='relu', input_dim=X_train_features.shape[1],\n",
    "                         kernel_initializer=keras.initializers.VarianceScaling(scale=2.0, seed=0)\n",
    "                         )\n",
    "                   )\n",
    "    # dropout for regularization\n",
    "    nn_2l_model.add(keras.layers.Dropout(0.5))\n",
    "    #second layer : softmax\n",
    "    nn_2l_model.add(Dense(6, activation='softmax',\n",
    "                         kernel_initializer=keras.initializers.VarianceScaling(scale=1.0, seed=0)\n",
    "                         )\n",
    "                   )\n",
    "    nn_2l_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    nn_1l_fit = nn_2l_model.fit(\n",
    "                            x=X_train_preprocessed, y=y_train_1h,\n",
    "                            validation_data=(X_valid_preprocessed, y_valid_1h), batch_size=32, epochs=50,\n",
    "                            callbacks=[keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)]\n",
    "                            ,shuffle=True\n",
    "                            )\n",
    "    (test_loss, test_accuracy) = nn_2l_model.evaluate(X_test_preprocessed, y_test_1h, batch_size=100)\n",
    "    print(\"Test Accuracy :\", test_accuracy)\n",
    "    nn2_test.append((units,test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32, 0.8999999761581421), (64, 0.9200000166893005), (128, 0.9200000166893005)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64 and 128 units gives the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1-layer neural networks gives best results than 2-layers neural network. This is due to the fact that our train dataset is small (1400 rows) and the input space dimension is high (2048 features). The training of our neural networks start overfitting after few epochs. \n",
    "\n",
    "To have better results to Train neural network, we needs generally much more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
